import logging
import random

import numpy as np
import torch
from omegaconf import DictConfig

log = logging.getLogger(__name__)
from sklearn.metrics import auc, precision_recall_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score


def seed_everything(seed: int):
    log.info("Global seed set to %d", seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    return seed


def compute_accuracy(gt, pred):
    return (((pred == gt).sum()) / gt.size(0)).item() * 100


def compute_auprc(gt, pred):
    return average_precision_score(gt, pred)


def compute_accuracy_np_array(gt, pred):
    return np.mean(gt == pred)


def convert_dictconfig_to_dict(cfg):
    if isinstance(cfg, DictConfig):
        return {k: convert_dictconfig_to_dict(v) for k, v in cfg.items()}
    else:
        return cfg

def pr_auc(gt, pred, get_all=False):
    precision, recall, _ = precision_recall_curve(gt, pred)
    score = auc(recall, precision)
    if get_all:
        return score, precision, recall
    else:
        return score

def pfbeta_binarized(gt, pred):
    positives = pred[gt == 1]
    scores = []
    for th in positives:
        binarized = (pred >= th).astype('int')
        score = pfbeta(gt, binarized, 1)
        scores.append(score)

    return np.max(scores)


def auroc(gt, pred):
    return roc_auc_score(gt, pred)


def pfbeta(gt, pred, beta):
    y_true_count = 0
    ctp = 0
    cfp = 0

    for idx in range(len(gt)):
        prediction = min(max(pred[idx], 0), 1)
        if (gt[idx]):
            y_true_count += 1
            ctp += prediction
            # cfp += 1 - prediction
        else:
            cfp += prediction

    beta_squared = beta * beta
    c_precision = ctp / (ctp + cfp)
    c_recall = ctp / y_true_count
    if c_precision > 0 and c_recall > 0:
        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)
        return result
    else:
        return 0


def pr_auc(gt, pred, get_all=False):
    precision, recall, _ = precision_recall_curve(gt, pred)
    score = auc(recall, precision)
    if get_all:
        return score, precision, recall
    else:
        return score
